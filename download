#!/Users/dsanford/anaconda/bin/python

import re
import sys
import requests
import pandas as pd
import json
from sqlalchemy import create_engine
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer


base_url = "https://en.wikipedia.org/wiki/Category:"

input_string = sys.argv[1]
search_terms = []

preprocessor = CountVectorizer().build_preprocessor()
tokenizer = CountVectorizer().build_tokenizer()

username = ""
password = ""

def sql_engine():
    fin = open("sql_credentials.txt")
    for line in fin:
        print line
        matches = re.search("(\w+) : (.+)",line)

        if matches is not None:
            if matches.group(1) is "username":
                username = matches.group(2)

                
            if matches.group(1) is "username":
                password = matches.group(2)



sql_engine()

quit()

for arg in sys.argv[1:]:
    if '.' in arg:
        fin = open(arg)
        for line in fin:
            match = re.match(r"[\W]+(\w.+)",line)
            if match != None:
                search_terms.append(match.group(1))

    else:
        match = re.match(r"(\w.+)",arg)
        if match != None:
            search_terms.append(match.group(1))

for term in search_terms:
    HTTP = "https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:{}&cmlimit=500&format=json".format(term)

    response = requests.get(HTTP)

    text_file = open("Physical_Quantities.json", "w")
    text_file.write(response.text)
    text_file.close()

    category_pages = \
        pd.DataFrame(json.loads(response.text)["query"]["categorymembers"])

    # Restrict to pages, dropping sub-categories
    category_pages = category_pages[category_pages['ns'] == 0]

    for i in range(min(category_pages.shape[0], 2)):

        page_code = category_pages.iloc[i,1]
        page_name = category_pages.iloc[i,2]

        print page_code,page_name
        HTTP = "https://en.wikipedia.org/w/api.php?action=parse&page={}&prop=sections&format=json".format(page_name)

        response = requests.get(HTTP)

        page_sections = \
            pd.DataFrame(json.loads(response.text)['parse']['sections'])

        print page_sections

        HTTP ="https://en.wikipedia.org/w/api.php?action=parse&format=json&page={}&prop=text&contentmodel=wikitext".format(page_name)

        response = requests.get(HTTP)
        extracted_text = json.loads(response.text)['parse']['text']['*']
        soup = BeautifulSoup(extracted_text)

        processed_text = preprocessor(preprocessor(soup.get_text()))
        merged_text = " ".join(tokenizer(processed_text.replace('\n',' ')))

        print merged_text

