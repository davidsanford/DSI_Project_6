#!/Users/dsanford/anaconda/bin/python

import re
import sys
import requests
import pandas as pd
import json
from sqlalchemy import create_engine
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer

import datetime

from common import sql_engine

if len(sys.argv) < 2:
    print "No arguments passed.  Need at least one category name or file with category list."
    quit()

preprocessor = CountVectorizer().build_preprocessor()
tokenizer = CountVectorizer().build_tokenizer()



def category_pages(category):

    HTTP = "https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:{}&cmlimit=500&titles={}&format=json".format(category,category)

    response = requests.get(HTTP)

    data_dict = json.loads(response.text)

    subject_pages = \
        pd.DataFrame(data_dict["query"]["categorymembers"])

    # Restrict to pages, dropping sub-categories
    if subject_pages.empty == False:
        subject_pages = subject_pages[subject_pages['ns'] == 0]

    category_dict = data_dict["query"]["pages"][data_dict["query"]["pages"].keys()[0]]

    categoryid = category_dict.get("pageid","")
    categoryname = category_dict.get("title","")

    return (categoryid, categoryname, subject_pages)



def page_info(page_code, page_name):

    HTTP ="https://en.wikipedia.org/w/api.php?action=parse&format=json&pageid={}&prop=text|sections&contentmodel=wikitext".format(page_code)

    response = requests.get(HTTP)

    data_dict = json.loads(response.text.encode('ascii', 'replace'))

    page_sections = \
        pd.DataFrame(data_dict['parse']['sections'])

    extracted_text = data_dict['parse']['text']['*']
    soup = BeautifulSoup(extracted_text, "lxml")

    processed_text = preprocessor(preprocessor(soup.get_text()))

    merged_text = " ".join(tokenizer(processed_text.replace('\n',' ')))

    return merged_text.encode('ascii', 'replace'), page_sections



def categories_from_file(filename):
    fin = open(filename)
    terms = []
    for line in fin:
        match = re.match(r"[\W]+(\w.+)",line)
        if match != None:
            terms.append(match.group(1))

    return terms



def category_list(term_list):

    categories = []

    for arg in term_list:
        if '.' in arg:
            categories = categories + categories_from_file(arg)

        else:
            match = re.match(r"(\w.+)",arg)
            if match != None:
                categories.append(match.group(1))  

    return categories



search_terms = category_list(sys.argv[1:])

engine = sql_engine()

for term in search_terms:

    categoryid, categoryname, pages = category_pages(term)
    
    if pages.empty is True or categoryid is "" or categoryname is "":
        print "ERROR : Category name \""+term+"\" does not exist" 
        continue

    
    #print pages

    name = str.replace(categoryname.encode('ascii', 'replace'), "\'", "\'\'")

    print name

    sql = "INSERT INTO category (category_id, category_name) VALUES ({}, {})".format(categoryid, "\'"+name+"\'")

    engine.execute(sql)

    for i in range(pages.shape[0]):
        info = page_info(pages['pageid'].iloc[i],
                         pages['title'].iloc[i])

        print "\t",pages['pageid'].iloc[i], pages['title'].iloc[i]
        #print info[0][:200],"\n"

        page_id = pages['pageid'].iloc[i]
        page_name = str.replace(pages['title'].iloc[i].encode('ascii', 'replace'), "\'", "\'\'")
        page_text = str.replace(info[0], "\'", "\'\'")
        
        sql = "INSERT INTO pages (page_id, category_id, title, page) VALUES ({}, {}, {}, {});".format(page_id, categoryid, "\'"+page_name+"\'", "\'"+page_text+"\'")

        engine.execute(sql)
